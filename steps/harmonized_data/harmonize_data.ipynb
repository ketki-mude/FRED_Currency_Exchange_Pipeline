{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "--!jinja\n",
    "ENV = '{{ env }}'\n",
    "print(\"Running notebook in environment:\", ENV)\n",
    "\n",
    "\n",
    "from snowflake.snowpark import Session\n",
    "from snowflake.snowpark import Session, functions as F\n",
    "from snowflake.snowpark.functions import col, min, max\n",
    "from snowflake.snowpark.window import Window\n",
    "from copy import copy\n",
    "\n",
    "session = Session.get_active_session()\n",
    "session.sql(f\"ALTER SESSION SET ENV = '{ENV}'\").collect()\n",
    "\n",
    "RAW_SCHEMA = f\"{ENV}_RAW_SCHEMA\"             # e.g., DEV_RAW_SCHEMA (where your 6 raw tables reside)\n",
    "\n",
    "HARMONIZED_SCHEMA = f\"{ENV}_HARMONIZED_SCHEMA\"   # Target schema for harmonized views/tables\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "create_udf_sql = f\"\"\"\n",
    "\n",
    "CREATE OR REPLACE FUNCTION {HARMONIZED_SCHEMA}.USD_CONVERSION_UDF(EXCHANGE_RATE NUMBER(35,4))\n",
    "\n",
    "RETURNS NUMBER(35,4)\n",
    "\n",
    "LANGUAGE SQL\n",
    "\n",
    "AS\n",
    "\n",
    "$$\n",
    "\n",
    "    CASE \n",
    "\n",
    "        WHEN EXCHANGE_RATE = 0 THEN 0\n",
    "\n",
    "        ELSE 1 / EXCHANGE_RATE\n",
    "\n",
    "    END\n",
    "\n",
    "$$;\n",
    "\n",
    "\"\"\"\n",
    " \n",
    "# Execute the SQL to create the UDF\n",
    "\n",
    "session.sql(create_udf_sql).collect()\n",
    " \n",
    "print(\"âœ… USD_CONVERSION_UDF created successfully!\")\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Define raw data mapping with table names matching those created earlier\n",
    "\n",
    "raw_data = {\n",
    "\n",
    "    \"RAW_DAILY\": [\"DEXINUS\", \"DEXUSEU\", \"DEXUSUK\"],\n",
    "\n",
    "    \"RAW_MONTHLY\": [\"EXINUS\", \"EXUSEU\", \"EXUSUK\"]\n",
    "\n",
    "}\n",
    " \n",
    "def fill_missing_dates(df):\n",
    "\n",
    "    # Assume df has columns DATE and VALUE (uppercase, as defined in your raw tables)\n",
    "\n",
    "    # Sort the dataframe by DATE\n",
    "\n",
    "    df = df.sort(\"DATE\")\n",
    "\n",
    "    # Get min and max date from the dataset\n",
    "\n",
    "    agg_row = df.agg(F.min(\"DATE\").alias(\"MIN_DATE\"), F.max(\"DATE\").alias(\"MAX_DATE\")).collect()[0]\n",
    "\n",
    "    min_date, max_date = agg_row[\"MIN_DATE\"], agg_row[\"MAX_DATE\"]\n",
    "\n",
    "    print(min_date, max_date)\n",
    " \n",
    "    # Calculate the number of days between min_date and max_date\n",
    "\n",
    "    date_diff_query = f\"SELECT DATEDIFF(DAY, '{min_date}', '{max_date}') AS DATE_DIFF\"\n",
    "\n",
    "    date_diff = session.sql(date_diff_query).collect()[0][\"DATE_DIFF\"]\n",
    " \n",
    "    # Generate full date range using Snowflake's GENERATOR function, aliasing the date as FULL_DATE\n",
    "\n",
    "    full_date_range_df = session.sql(f\"\"\"\n",
    "\n",
    "        SELECT CAST(DATEADD(DAY, SEQ4(), '{min_date}') AS DATE) AS FULL_DATE\n",
    "\n",
    "        FROM TABLE(GENERATOR(ROWCOUNT => {date_diff + 1}))\n",
    "\n",
    "    \"\"\")\n",
    "\n",
    "    # Alias the dataframes to disambiguate the DATE columns\n",
    "\n",
    "    fdr = full_date_range_df.alias(\"fdr\")\n",
    "\n",
    "    d = df.alias(\"d\")\n",
    "\n",
    "    # Left join on full date (FULL_DATE) equals the raw DATE\n",
    "\n",
    "    joined_df = fdr.join(d, fdr[\"FULL_DATE\"] == d[\"DATE\"], how=\"left\")\n",
    "\n",
    "    # Define window specification based on FULL_DATE from the full date range\n",
    "\n",
    "    window_spec = Window.order_by(fdr[\"FULL_DATE\"]).rows_between(Window.UNBOUNDED_PRECEDING, Window.CURRENT_ROW)\n",
    "\n",
    "    # Fill missing VALUEs using last non-null value over the window\n",
    "\n",
    "    # We use the column from d (raw data) which may be null if there's no matching row\n",
    "\n",
    "    filled_df = joined_df.with_column(\n",
    "\n",
    "        \"FILLED_VALUE\",\n",
    "\n",
    "        F.coalesce(d[\"VALUE\"], F.last_value(d[\"VALUE\"], True).over(window_spec))\n",
    "\n",
    "    )\n",
    "\n",
    "    # Select only the FULL_DATE (renamed to DATE) and FILLED_VALUE (renamed to VALUE), and sort by date\n",
    "\n",
    "    final_df = filled_df.select(fdr[\"FULL_DATE\"].alias(\"DATE\"), F.col(\"FILLED_VALUE\").alias(\"VALUE\")).sort(\"DATE\")\n",
    "\n",
    "    return final_df\n",
    " \n",
    "def create_harmonized_view():\n",
    "\n",
    "    # Switch to harmonized schema\n",
    "\n",
    "    session.use_schema(HARMONIZED_SCHEMA)\n",
    " \n",
    "    for schema_name, tables in raw_data.items():\n",
    "\n",
    "        base_df = None  # Initialize base DataFrame\n",
    " \n",
    "        for table in tables:\n",
    "\n",
    "            # Use table names as stored in RAW_SCHEMA\n",
    "\n",
    "            table_suffix = table  # Here, table is already like \"DEXINUS\", etc.\n",
    "\n",
    "            # Select DATE and VALUE columns from the raw table\n",
    "\n",
    "            df = session.table(f\"{RAW_SCHEMA}.{table}\").select(F.col(\"DATE\"), F.col(\"VALUE\"))\n",
    " \n",
    "            # Fill missing dates and alias columns\n",
    "\n",
    "            if schema_name == \"RAW_DAILY\":\n",
    "\n",
    "                df = fill_missing_dates(df).select(F.col(\"DATE\").alias(\"DDATE\"), F.col(\"VALUE\").alias(table_suffix))\n",
    "\n",
    "                # Use a copy to avoid self-join issues\n",
    "\n",
    "                df_copy = copy(df)\n",
    "\n",
    "                base_df = df_copy if base_df is None else base_df.join(df_copy, on=\"DDATE\", how=\"outer\")\n",
    "\n",
    "            else:\n",
    "\n",
    "                # For monthly data, simply alias the columns\n",
    "\n",
    "                df = df.select(F.col(\"DATE\").alias(\"MDATE\"), F.col(\"VALUE\").alias(table_suffix))\n",
    "\n",
    "                df_copy = copy(df)\n",
    "\n",
    "                base_df = df_copy if base_df is None else base_df.join(df_copy, on=\"MDATE\", how=\"outer\")\n",
    " \n",
    "        # Apply UDF conversion on specific columns\n",
    "\n",
    "        if schema_name == \"RAW_DAILY\":\n",
    "\n",
    "            base_df = base_df.with_column(\"DEXUSEU_CONVERTED\", F.call_udf(f\"{HARMONIZED_SCHEMA}.USD_CONVERSION_UDF\", F.col(\"DEXUSEU\")))\n",
    "\n",
    "            base_df = base_df.with_column(\"DEXUSUK_CONVERTED\", F.call_udf(f\"{HARMONIZED_SCHEMA}.USD_CONVERSION_UDF\", F.col(\"DEXUSUK\")))\n",
    "\n",
    "        else:\n",
    "\n",
    "            base_df = base_df.with_column(\"EXUSEU_CONVERTED\", F.call_udf(f\"{HARMONIZED_SCHEMA}.USD_CONVERSION_UDF\", F.col(\"EXUSEU\")))\n",
    "\n",
    "            base_df = base_df.with_column(\"EXUSUK_CONVERTED\", F.call_udf(f\"{HARMONIZED_SCHEMA}.USD_CONVERSION_UDF\", F.col(\"EXUSUK\")))\n",
    " \n",
    "        # Create a harmonized view. The view name will be something like HARMONIZED_DAILY_V or HARMONIZED_MONTHLY_V.\n",
    "\n",
    "        view_name = f\"HARMONIZED_{schema_name.split('_')[1]}_V\".upper()\n",
    "\n",
    "        base_df.create_or_replace_view(view_name)\n",
    "\n",
    "        print(f\"âœ… {view_name} created successfully!\")\n",
    " \n",
    "def create_harmonized_stream():\n",
    "\n",
    "    session.use_schema(HARMONIZED_SCHEMA)\n",
    "\n",
    "    for schema_name in raw_data.keys():\n",
    "\n",
    "        if schema_name == \"RAW_DAILY\":\n",
    "\n",
    "            session.sql(\"\"\"\n",
    "\n",
    "                CREATE OR REPLACE TABLE HARMONIZED_DAILY_TBL AS \n",
    "\n",
    "                SELECT * FROM HARMONIZED_DAILY_V\n",
    "\n",
    "            \"\"\").collect()\n",
    "\n",
    "            session.sql(\"\"\"\n",
    "\n",
    "                CREATE OR REPLACE STREAM HARMONIZED_DAILY_STREAM \n",
    "\n",
    "                ON TABLE HARMONIZED_DAILY_TBL \n",
    "\n",
    "                SHOW_INITIAL_ROWS = TRUE\n",
    "\n",
    "            \"\"\").collect()\n",
    "\n",
    "            print(\"Daily Stream created successfully!\")\n",
    "\n",
    "        else:\n",
    "\n",
    "            session.sql(\"\"\"\n",
    "\n",
    "                CREATE OR REPLACE TABLE HARMONIZED_MONTHLY_TBL AS \n",
    "\n",
    "                SELECT * FROM HARMONIZED_MONTHLY_V\n",
    "\n",
    "            \"\"\").collect()\n",
    "\n",
    "            session.sql(\"\"\"\n",
    "\n",
    "                CREATE OR REPLACE STREAM HARMONIZED_MONTHLY_STREAM \n",
    "\n",
    "                ON TABLE HARMONIZED_MONTHLY_TBL \n",
    "\n",
    "                SHOW_INITIAL_ROWS = TRUE\n",
    "\n",
    "            \"\"\").collect()\n",
    "\n",
    "            print(\"Monthly Stream created successfully!\")\n",
    " \n",
    "# Execute the functions\n",
    "\n",
    "create_harmonized_view()\n",
    "\n",
    "create_harmonized_stream()\n",
    "\n",
    "# If you have a combined table function defined, call it here:\n",
    "\n",
    "# create_harmonized_combined_table()\n",
    " \n",
    "print(\"ðŸ”„ Harmonization process completed.\")\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Execute the functions\n",
    "create_harmonized_view()\n",
    "create_harmonized_stream()\n",
    "create_harmonized_combined_table()\n",
    "\n",
    "print(\"ðŸ”„ Harmonization process completed.\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
