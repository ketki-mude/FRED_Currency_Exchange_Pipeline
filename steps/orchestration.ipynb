{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Import python packages\n",
    "from snowflake.core import Root\n",
    "\n",
    "# We can also use Snowpark for our analyses!\n",
    "from snowflake.snowpark.context import get_active_session\n",
    "session = get_active_session()\n",
    "\n",
    "# Set up your role and warehouse\n",
    "session.use_role(\"FRED_ROLE\")  # Using FRED_ROLE as specified\n",
    "session.use_warehouse(\"FRED_WH\")\n",
    "\n",
    "# Define your database and schema\n",
    "database_name = \"FRED_DB\"\n",
    "schema_name = \"{{env}}_SCHEMA\"  # This will be replaced by Jinja templating\n",
    "env = \"{{env}}\"  # This will be replaced by Jinja templating\n",
    "\n",
    "session.use_schema(f\"{database_name}.{schema_name}\")\n",
    "\n",
    "# Import DAG-related modules\n",
    "from snowflake.core.task.dagv1 import DAGOperation, DAG, DAGTask\n",
    "from datetime import timedelta\n",
    "\n",
    "# Create the tasks using the DAG API\n",
    "warehouse_name = \"FRED_WH\"\n",
    "dag_name = f\"{env}_FRED_CURRENCY_PIPELINE\"\n",
    "\n",
    "api_root = Root(session)\n",
    "schema = api_root.databases[database_name].schemas[schema_name]\n",
    "dag_op = DAGOperation(schema)\n",
    "\n",
    "# Define the DAG\n",
    "with DAG(dag_name, schedule=timedelta(days=1), warehouse=warehouse_name) as dag:\n",
    "    # Define tasks for each step in the pipeline\n",
    "    # Each task executes a specific notebook for that stage of the pipeline\n",
    "    \n",
    "    # 1. Load Raw Data Task - Executes the load_raw_data notebook\n",
    "    load_raw_data_task = DAGTask(\n",
    "        \"LOAD_RAW_DATA_TASK\", \n",
    "        definition=f'''EXECUTE NOTEBOOK \"{database_name}\".\"{env}_SCHEMA\".\"{env}_load_raw_data\"()''', \n",
    "        warehouse=warehouse_name\n",
    "    )\n",
    "    \n",
    "    # 2. Harmonize Data Task - Executes the harmonize_data notebook\n",
    "    harmonize_data_task = DAGTask(\n",
    "        \"HARMONIZE_DATA_TASK\", \n",
    "        definition=f'''EXECUTE NOTEBOOK \"{database_name}\".\"{env}_SCHEMA\".\"{env}_harmonize_data\"()''', \n",
    "        warehouse=warehouse_name\n",
    "    )\n",
    "    \n",
    "    # 3. Analytics Task - Executes the analytics notebook\n",
    "    analytics_task = DAGTask(\n",
    "        \"ANALYTICS_TASK\", \n",
    "        definition=f'''EXECUTE NOTEBOOK \"{database_name}\".\"{env}_SCHEMA\".\"{env}_analytics\"()''', \n",
    "        warehouse=warehouse_name\n",
    "    )\n",
    "\n",
    "    # Define the dependencies between the tasks\n",
    "    load_raw_data_task >> harmonize_data_task >> analytics_task\n",
    "\n",
    "# Create the DAG in Snowflake\n",
    "dag_op.deploy(dag, mode=\"orreplace\")\n",
    "\n",
    "# List all DAGs with similar names\n",
    "dagiter = dag_op.iter_dags(like=f'{env}_FRED_CURRENCY_PIPELINE%')\n",
    "for dag_name in dagiter:\n",
    "    print(dag_name)\n",
    "\n",
    "# Uncomment to run the DAG immediately\n",
    "# dag_op.run(dag)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
